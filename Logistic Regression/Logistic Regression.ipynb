{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EUPS Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is quite similar to linear regression. While linear regression is used to predict values like height or cost, logistic regression tends to be used as a *classifier*, that predicts which of two classes an input data sample is more likely to be, i.e. mammal or reptile, usually represented by 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $x_1$ | $x_2$ | $y$(Class) |\n",
    "| ----- | ----- | --- |\n",
    "|  2    | 3     |  0  |\n",
    "|  1    | 4     |  1  |\n",
    "|  0.2    | -2     |  1  |\n",
    "|  5    | 5     |  0  |\n",
    "\n",
    "*Note - these are dummy numbers...don't look for any meaning in them or their relationship!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Error\n",
    "\n",
    "We cannot use the same error measure as linear regression. The error used here is called *cross-entropy*.\n",
    "\n",
    "\\begin{equation}\n",
    "Error = - \\sum_{i=1}^N y_ilog(p_i) + (1-y_i)log(1-p_i) \\\\\n",
    "\\text{,where } p_i = predictions_i\n",
    "\\end{equation}\n",
    "\n",
    "Code an `error(predictions,ys)` function that takes in a **column vector** (shape $n \\times 1$) of predictions and a **column vector** of actual values and computes the error as described, returning a single number just like linear regression. The $i$th row in the prediction column vector corresponds to the prediction for the $i$th data input.\n",
    "\n",
    "Useful `numpy` functions:\n",
    "\n",
    "`np.log`, `np.sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input:\n",
    "# predictions_example = np.array([[0.8],[0.06],[0.6],[0.98]])\n",
    "# ys_example = np.array([[1],[0],[1],[1]])\n",
    "# Expected ouput : 0.8160472861158075\n",
    "# Write your code below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Prediction\n",
    "\n",
    "The prediction is very similar to linear regression. Basically you just do exactly the same as linear regression and then pump the prediction value you get from that through a special function called the *sigmoid activation function* $\\sigma$. \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma \\left(x \\right) = \\frac{1}{1+e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "So the prediction of one data point $\\mathbf{x} = \\begin{pmatrix}x_1 & x_2 & ... & x_n\\end{pmatrix}$ for logistic regression is just this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma \\left(b + w_1x_1 + w_2x_2 + \\cdots + w_nx_n \\right) \\\\\n",
    "= \\sigma \\left(b + \\sum_{i=1}^N w_ix_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "First code a function `sigmoid(x)` which takes in a numpy array of values and applies the $\\sigma$ function to them.\n",
    "\n",
    "Useful `numpy` functions:\n",
    "\n",
    "`np.exp()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write sigmoid here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use that to code a `predict(X,ws,b)` exactly as you did for linear regression but using the new prediction formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example input:\n",
    "# X_example = np.array([\n",
    "# [2,5],\n",
    "# [1,6],\n",
    "# [-2,5]\n",
    "# ])\n",
    "# ws_example = np.array([[7.32,1.11]])\n",
    "# b_example = 5\n",
    "# Expected output : array([[1.        , 0.99999999, 0.01646364]])\n",
    "# Write predict below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Learning\n",
    "\n",
    "The procedure is identical to linear regression, but now the partial derivative terms are different because the prediction formula has changed. Try and calculate the terms yourself before checking with ours.  The terms we need to calculate for gradient descent are the derivative of the error (above) w.r.t. the bias term and also w.r.t. a single weight term, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial Error}{\\partial b} \\\\\n",
    "\\frac{\\partial Error}{\\partial w_i}\n",
    "\\end{equation}\n",
    "\n",
    " Using your results, write a function `update(b, ws, X, predictions, ys, learning_rate = 0.001)`. The only new term here is the learning rate, which appears in the gradient descent formula. The function should return the new weights and bias after updating them with one step of gradient descent. As a reminder, here are the formulae:\n",
    "\n",
    "\\begin{equation}\n",
    "b_{new} = b_{old} - \\eta \\frac{\\partial Error}{\\partial b_{old}} \\\\\n",
    "w_{i,new} = w_{i,old} - \\eta \\frac{\\partial Error}{\\partial w_{i,old}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code update here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - All Together\n",
    "\n",
    "All the groundwork for the algorithm has been laid, now for the easy part of stitching it all together. Write a function, `fit(X,y,epochs)`, that takes in a numpy array of input features for a certain number of samples, and also the correct output values for the number of samples and runs the linear regression algorithm on them, by running gradient descent `epochs` number of times. It must :\n",
    "\n",
    "- Initialise some random weights, the same number as there are features.\n",
    "- Run the following each epoch:\n",
    "    - Call your `predict` function to get a set of predictions for the input features.\n",
    "    - Call `error` on the result of your predictions to compare how good your predictions were.\n",
    "    - Print out your error so we can hopefully see it reducing!\n",
    "    - Update the weights using your `update` function.\n",
    "- Finally return the numpy array corresponding to the trained weights and the bias term from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code fit here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Terms\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial Error}{\\partial b} = - \\sum_{i=1}^N y_i(1-p_i) - (1-y_i)p_i\\\\\n",
    "\\frac{\\partial Error}{\\partial w_j} = \\sum_{i=1}^N x_{ij}y_i(1-p_i) - x_{ij}(1-y_i)p_i\n",
    "\\end{equation}\n",
    "\n",
    "The sums in both expressions sum over the training examples (samples)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
