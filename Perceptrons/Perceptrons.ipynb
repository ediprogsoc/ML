{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "Now that we've looked at linear regression and logistic regression, we can start learning about the fundamentals of neural networks! The simplest neural network is the **perceptron**, and it was developed by Frank Rosenblatt in 1957. The perceptron is a **binary classifier**, which is an algorithm that can classify data across two categories. In technical terms, the perceptron is a form of **supervised learning**, where we 'teach' a machine to learn some mapping between a set of input values to a set of outputs through labelled training data.\n",
    "\n",
    "## Theory\n",
    "\n",
    "The perceptron is an artifical neuron, in the sense that it is modelled after the biological neurons found in our brains. Below is a diagram of a biological neuron: \n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Neuron.svg/640px-Neuron.svg.png)\n",
    "\n",
    "A biological neuron receives input signals from other neurons through its dendrites. These signals are acculumated in the cell body. If the accumulated signal exceeds a certain threshold, the neuron will 'activate' and fire an output signal which is transmitted through its axon. \n",
    "\n",
    "The key insights that computer scientists draw from the biological neuron is its ability to receive input, accumulate this input, then produce an output based on whether or not the accumulated input exceeds a particular threshold. Bearing these insights in mind, let's look at the architecture of a perceptron:\n",
    "\n",
    "![title](https://www.researchgate.net/profile/Zafeirios_Fountas/publication/266485234/figure/fig2/AS:651919710224385@1532441179639/The-simplest-mathematical-model-of-a-neuron-called-the-Perceptron-30.png)\n",
    "\n",
    "The perceptron takes in a number of inputs and calculates the weighted sum of those inputs. In a biological neuron, this step corresponds to the accumulation of the input signals in the cell body. Once the weighted sum has been calculated, we check if it exceeds a certain threshold. If it does, we fire or 'activate' the perceptron to produce an output. This step is implemented in the **activation function** of the perceptron. There are many possible activation functions we can choose from to implement in our perceptron. An example would be the sigmoid activation function, which you have come across already when you learned about logistic regression. But to keep things simple for now, we will implement what is known as the **unit-step** activation function, which looks like this:\n",
    "\n",
    "![title](https://miro.medium.com/max/762/1*K9QJmeG33SvQeJgPa52mmQ.png)\n",
    "\n",
    "As we can see, after a certain threshold (in this case 0) the function immediately jumps to 1 and 'activates', just like a biological neuron! \n",
    "\n",
    "So we now know the basic structure of a perceptron. But what is the input, and what is the output exactly? The input is the data we use to classify whatever it is we're trying to classify ($x_{i}$), and the output is the perceptron's prediction of what the class of the input should be (1 or 0). More specifically, the input is a **feature vector**, meaning it is a vector that contains some information about the object we are classifying. For instance, if we were trying to determine whether a dog is a mammal or a bird, we might use some data about its features to help us. Dogs have four legs, and unlike birds they don't have wings. From data like this, a perceptron might make the prediction that dogs are mammals. Of course, it would be right, and it would seem like our job is done. But what if it was wrong?\n",
    "\n",
    "This is where **weights** come in. Weights are the essential mechanism behind a perceptron's ability to 'learn'. One weight is assigned to each feature in the input feature vector, and each weight represents the relative importance of that feature to the classification of our input object. Remember that the perceptron calculates a weighted sum to determine whether or not it should activate. Since this sum is dependent on the perceptron's weights, a change in even one weight might make the difference in causing the perceptron to activate or not. As a result, the task of 'learning' becomes a matter of adjusting the perceptron's weights accordingly to minimise its  errors in classification. In other words, when we 'train' a perceptron, we are simply letting it automatically learn or find the configuration of weights that yields the most accurate classifications of the training data in order to prepare it well when it eventually has to classify unseen data. Importantly, the threshold of the activation function itself can be treated as a weight. Formally, since the activation function is $z \\geq \\theta$ (where z is the weighted sum and $\\theta$ is the threshold) we can rearrange this so that $z + (-\\theta*1) \\geq 0$. The $(-\\theta*1)$ term is referred to as the **bias**.   \n",
    "\n",
    "So how does a perceptron adjust its weights? Rosenblatt designed the **perceptron learning rule** for this:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta w_{j} = \\eta(target_{i} - output_{i})x_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is the **learning rate** of the perceptron, $target_{i}$ is the target sample, $output_{i}$ is the perceptron's prediction and $x_{ij}$ is the feature input associated with the weight we are adjusting. From this formula, we can see that if the perceptron's prediction is the same as the target value (i.e both are either 1 or 0), then $\\Delta w_{j}$ will be 0 meaning the weight does not need to be adjusted at all. This makes sense since the perceptron's prediction was already correct! But if the perceptron was wrong (e.g. its prediction was 1 when the target is 0 or its prediction was 0 when the target is 1), then the weights would be adjusted to produce a result that is closer to the target, since $target_{i}-output_{i}$ will either be 1 or -1. As for the learning rate of the perceptron, this is a constant which acts as a multiplier for $\\Delta w_{j}$. The higher $\\eta$ is, the higher $\\Delta w_{j}$ will be.  \n",
    "\n",
    "\n",
    "Let's now move on to the code so we can implement our own perceptron! \n",
    "\n",
    "## Let's code!\n",
    "\n",
    "### The Iris dataset\n",
    "\n",
    "Before implementing our perceptron, we need some data. The Iris dataset is a small dataset which contains some information about 150 various iris plants of three types: setosa, versicolor and virginica. The first 50 samples in the dataset are setosa samples, the next 50 are versicolor, and the final 50 are virginica. For our purposes, we will focus on classifying whether an iris is setosa or versicolor, and ignore the data on virginica irises. The irises have a four features which can help us classify them. These are petal length, petal width, sepal length, and sepal width. To better visualise the linear classification problem, we will classify the irises based on only two of their features. This gives us a two-dimensional feature vector which is easy to plot!  \n",
    "\n",
    "First, let's import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the Iris dataset, and only take the information about the sepal length and petal length of each iris. X is the data we will train the perceptron on, and y is the array of target values we will refer to in order to adjust the weights of our model. \n",
    "\n",
    "Important information:\n",
    "\n",
    "- Each item in x is a numpy array consisting of 2 values: the sepal length and petal length of the training sample **in that order**. \n",
    "- In y, 0 represents that the ith sample is a setosa iris, while 1 represents that it is a versicolor iris. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Setosa data:\n",
      "\n",
      "    sepal length (cm)  petal length (cm)\n",
      "0                 5.1                1.4\n",
      "1                 4.9                1.4\n",
      "2                 4.7                1.3\n",
      "3                 4.6                1.5\n",
      "4                 5.0                1.4\n",
      "5                 5.4                1.7\n",
      "6                 4.6                1.4\n",
      "7                 5.0                1.5\n",
      "8                 4.4                1.4\n",
      "9                 4.9                1.5\n",
      "10                5.4                1.5\n",
      "11                4.8                1.6\n",
      "12                4.8                1.4\n",
      "13                4.3                1.1\n",
      "14                5.8                1.2\n",
      "15                5.7                1.5\n",
      "16                5.4                1.3\n",
      "17                5.1                1.4\n",
      "18                5.7                1.7\n",
      "19                5.1                1.5\n",
      "20                5.4                1.7\n",
      "21                5.1                1.5\n",
      "22                4.6                1.0\n",
      "23                5.1                1.7\n",
      "24                4.8                1.9\n",
      "25                5.0                1.6\n",
      "26                5.0                1.6\n",
      "27                5.2                1.5\n",
      "28                5.2                1.4\n",
      "29                4.7                1.6\n",
      "30                4.8                1.6\n",
      "31                5.4                1.5\n",
      "32                5.2                1.5\n",
      "33                5.5                1.4\n",
      "34                4.9                1.5\n",
      "35                5.0                1.2\n",
      "36                5.5                1.3\n",
      "37                4.9                1.4\n",
      "38                4.4                1.3\n",
      "39                5.1                1.5\n",
      "40                5.0                1.3\n",
      "41                4.5                1.3\n",
      "42                4.4                1.3\n",
      "43                5.0                1.6\n",
      "44                5.1                1.9\n",
      "45                4.8                1.4\n",
      "46                5.1                1.6\n",
      "47                4.6                1.4\n",
      "48                5.3                1.5\n",
      "49                5.0                1.4\n",
      "\n",
      "Versicolor data:\n",
      "\n",
      "    sepal length (cm)  petal length (cm)\n",
      "50                7.0                4.7\n",
      "51                6.4                4.5\n",
      "52                6.9                4.9\n",
      "53                5.5                4.0\n",
      "54                6.5                4.6\n",
      "55                5.7                4.5\n",
      "56                6.3                4.7\n",
      "57                4.9                3.3\n",
      "58                6.6                4.6\n",
      "59                5.2                3.9\n",
      "60                5.0                3.5\n",
      "61                5.9                4.2\n",
      "62                6.0                4.0\n",
      "63                6.1                4.7\n",
      "64                5.6                3.6\n",
      "65                6.7                4.4\n",
      "66                5.6                4.5\n",
      "67                5.8                4.1\n",
      "68                6.2                4.5\n",
      "69                5.6                3.9\n",
      "70                5.9                4.8\n",
      "71                6.1                4.0\n",
      "72                6.3                4.9\n",
      "73                6.1                4.7\n",
      "74                6.4                4.3\n",
      "75                6.6                4.4\n",
      "76                6.8                4.8\n",
      "77                6.7                5.0\n",
      "78                6.0                4.5\n",
      "79                5.7                3.5\n",
      "80                5.5                3.8\n",
      "81                5.5                3.7\n",
      "82                5.8                3.9\n",
      "83                6.0                5.1\n",
      "84                5.4                4.5\n",
      "85                6.0                4.5\n",
      "86                6.7                4.7\n",
      "87                6.3                4.4\n",
      "88                5.6                4.1\n",
      "89                5.5                4.0\n",
      "90                5.5                4.4\n",
      "91                6.1                4.6\n",
      "92                5.8                4.0\n",
      "93                5.0                3.3\n",
      "94                5.6                4.2\n",
      "95                5.7                4.2\n",
      "96                5.7                4.2\n",
      "97                6.2                4.3\n",
      "98                5.1                3.0\n",
      "99                5.7                4.1\n"
     ]
    }
   ],
   "source": [
    "iris_data = load_iris(return_X_y=True, as_frame=True)\n",
    "print('\\nSetosa data:\\n')\n",
    "print(iris_data[0].iloc[:50, [0,2]])\n",
    "print('\\nVersicolor data:\\n')\n",
    "print(iris_data[0].iloc[50:100, [0,2]])\n",
    "X = iris_data[0].iloc[:100, [0,2]].to_numpy()\n",
    "y = [x for x in iris_data[1] if x == 0 or x == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below plots the data above in a graph with matplotlib. If you're interested in learning how to draw plots in Python, matplotlib is a great place to start! As we can see, the setosa and versicolor samples can easily be partitioned into two categories by a line! This means that the data we're working with is **linearly separable**.   Our peceptron's job is to find a line such that the data is separated into two classes with the highest accuracy possible. If our data is not linearly separable, then the perceptron will not **converge**, which just means it will have failed to find a solution to classify or **linearly separate** the training data perfectly. So, for future reference, to maximise the utility of a perceptron, we should make sure that the data we want to classify is linearly separable first.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "setosa_x = [x[0] for x in X[:50]]\n",
    "setosa_y = [x[1] for x in X[:50]]\n",
    "versicolor_x = [x[0] for x in X[50:100]]\n",
    "versicolor_y = [x[1] for x in X[50:100]]\n",
    "\n",
    "def calculate_ys(m, c, xs):\n",
    "    ys = [(m*x)+c for x in xs]\n",
    "    return ys\n",
    "\n",
    "def plot_figure(x1, y1, x2, y2):\n",
    "    plt.scatter(x1, y1, color='red', marker='x', label='setosa')\n",
    "    plt.scatter(x2, y2, color='blue', marker='o', label='versicolor')\n",
    "    axes = plt.gca()\n",
    "    plt.plot(np.arange(int(axes.get_xlim()[0]), int(axes.get_xlim()[1]+1)), \n",
    "             calculate_ys(0.12,2,np.arange(int(axes.get_xlim()[0]), int(axes.get_xlim()[1]+1))))\n",
    "    plt.xlabel('Sepal length (cm)')\n",
    "    plt.ylabel('Petal length (cm)')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "plot_figure(setosa_x, setosa_y, versicolor_x, versicolor_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, a `CustomPerceptron` class has been defined, with some skeleton code. It is up to you to implement the `activate` and `fit` functions. In the `fit` function, you should also count the number of errors the model makes every epoch, and store it in the `errors` variable of the class. After the perceptron has been trained, you can then plot the number of errors per epoch with the `plot_errors` function to see the model converge. Ask our tutors for help if you need any!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPerceptron:\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    # DO NOT TOUCH\n",
    "    \n",
    "    def __init__(self, b, lr, e):\n",
    "        self.bias = -b \n",
    "        self.learning_rate = lr\n",
    "        self.epochs = e\n",
    "        self.W = None\n",
    "        self.errors = []\n",
    "        \n",
    "    def get_weights(self):\n",
    "        return 'the weights of the model are currently {}\\n'.format(self.W)\n",
    "    \n",
    "    def activate_test(self, x):\n",
    "        labels = ['Setosa', 'Versicolor']\n",
    "        activate_result = 0\n",
    "        weighted_sum = np.dot(x, self.W) + self.bias\n",
    "        if weighted_sum >= 0:\n",
    "            activate_result = 1\n",
    "        print('Perceptron\\'s prediction: {}\\n'.format(labels[activate_result]))\n",
    "        \n",
    "    def plot_errors(self):\n",
    "        x = [i+1 for i in range(self.epochs)]\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.plot(x, self.errors, color='blue', marker='o')\n",
    "        plt.xlim(1, self.epochs)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('No. of misclassifications')\n",
    "    \n",
    "    #########################################################\n",
    "    \n",
    "    def activate(self, x):\n",
    "        # TODO\n",
    "        pass\n",
    "                \n",
    "    def fit(self, X, y):\n",
    "        # randomly initialise the weights\n",
    "        self.W = np.ones(X.shape[1])\n",
    "        for e in range(self.epochs):\n",
    "            print('Epoch {}: {}'.format(e+1, self.get_weights()))\n",
    "            \n",
    "            # TODO\n",
    "            # for each training sample, \n",
    "            # generate a prediction;\n",
    "            # calculate delta w_j;\n",
    "            # update the weights and bias;\n",
    "            # keep track of errors\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the CustomPerceptron class has been instantiated in the variable `custom_p`, with some default parameters. Fit the model to the training data and plot the number of errors in each epoch. Feel free to play around with these parameters to see how the weights and errors change!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default params: bias = 0, learning rate = 0.1, epochs = 10\n",
    "\n",
    "custom_p = CustomPerceptron(0, 0.1, 10)\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing!\n",
    "\n",
    "Now that we've trained the perceptron, we can use it to classify some new samples it hasn't seen before! Below are 5 new flowers. Use the `activate_test` function from the CustomPerceptron class to make these predictions. Remember that each sample is represented as a numpy array consisting of two features, the first being the sample's sepal length and the second being its petal length. \n",
    "\n",
    "Some questions to keep in mind:\n",
    "\n",
    "- Does the perceptron classify the new samples in the way you would expect? i.e. if you were to classify the new samples as setosa and versicolor based on the data we have from the Iris dataset, would it be the same as the perceptron's prediction?\n",
    "- Consider examples 4 and 5. How does the perceptron classify them and why has it classified them that way?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flower_1 = np.array([7.4, 4.2])\n",
    "\n",
    "new_flower_2 = np.array([4.3, 1.6])\n",
    "\n",
    "new_flower_3 = np.array([4.8, 1.7])\n",
    "\n",
    "new_flower_4 = np.array([6.8, 1.6])\n",
    "\n",
    "new_flower_5 = np.array([5.8, 4.6])\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
